Evaluation Methodology for RAG System

1. Retrieval evaluation: 

To evaluate my RAG system, I would begin with retrieval evaluation by computing recall@k and mean reciprocal rank (MRR), following the guidelines in the RGAR Survey (2024) and ARES (2024). These metrics quantify how often the relevant chunks from the	PDF appear among the top-k results returned by my FAISS retriever and how highly they are ranked. To further assess semantic relevance, I would employ an LLM-based judge-using Flan-T5 with prompts such as 'Does this chunk answer question? Respond Yes/No' and compare its scores to human annotations, as demonstrated in ARES. Robustness of the retriever would be tested by introducing out of topic queries, such as 'Explain action potential, for non biology papers' and ensuring that the system does not return irrelevant context, as emphasized by Es et al. (2024).

2. Generation evaluation:
For the generation component, I would focus on faithfulness by applying both automated and LLM-based methods. Automated evaluation would involve computing ROUGE-L or BLEU scores between generated answers and ground-truth fragments extracted from the PDF, flagging answers with low overlap for further review. In parallel, I would use Flan-T5 as an LLM-based judge to perform claim-level entailment checks, prompting the model with 'Is [claim] supported by [context]? Respond Yes/No' and aggregating the results as recommended in ARES. I would also rate answer relevance and completeness on a scale, ensuring that responses fully address the user's question. To systematically detect hallucinations, I would flag any answer containing facts, entities, or claims absent from the retrieved chunks, in line with the RGAR Survey's recommendations.

Finally, end-to-end and robustness testing would include submitting unanswerable queries to verify that the system responds with	'I donâ€™t know' as required by the prompt. I would monitor practical metrics such as latency (aiming for under five seconds per query) and token usage (limiting Flan-T5 to 300 tokens per query), adjusting chunk size and overlap if necessary. To complement automated metrics, I would conduct a small-scale human evaluation as described by Es et al. (2024), where domain experts score a stratified sample of responses for relevance, and I would report inter-rater reliability.

References:

1. Es, S., et al. (2024). A Methodology for Evaluating RAG Systems: A Case Study on Configuration Dependency Validation. arXiv:2410.08801.

2. RGAR Survey (2024). Evaluation of Retrieval-Augmented Generation: A Survey. arXiv:2405.07437.

3. Zhang, R., et al. (2024). ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation. Proceedings of NAACL.
